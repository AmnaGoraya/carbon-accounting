{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pycountry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# versions\n",
    "EXIOBASE_VERSION = '3.8.2'\n",
    "IGNITE_VERSION = '1.1'\n",
    "\n",
    "# Rest of world regions as defined in Exiobase\n",
    "ROW_AFRICA = 'Rest of World Africa'\n",
    "ROW_AMERICA = 'Rest of World America'\n",
    "ROW_ASIA = 'Rest of World Asia and Pacific'\n",
    "ROW_EUROPE = 'Rest of World Europe'\n",
    "ROW_ME = 'Rest of World Middle East'\n",
    "\n",
    "# Ignite column names\n",
    "EF_CAT_L1 = 'EF Category L1'\n",
    "EF_CAT_L2 = 'EF Category L2'\n",
    "EF_REGION = 'EF Region'\n",
    "EF_YEAR = 'EF Year'\n",
    "EF_CURR = 'EF Currency'\n",
    "EF_SOURCE = 'EF Source'\n",
    "EF_UNIQUE = 'EF Unique String'\n",
    "ADJUSTED = 'Adjusted by Ignite'\n",
    "IPCC_AR5 = 'EF IPCC AR5 [kg CO2e/€]'\n",
    "WATER = 'Water Consumption [m3/€]'\n",
    "LAND = 'Land Use [m2/€]'\n",
    "EMISSION_FACTOR = 'Emission Factor [kg CO2e/€]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers_from_region(region):\n",
    "    return region[0:2]\n",
    "\n",
    "def add_full_region_name(alpha_2):\n",
    "    alpha_2 = remove_numbers_from_region(alpha_2)\n",
    "    \n",
    "    if alpha_2 == 'WF':\n",
    "        return ROW_AFRICA\n",
    "    elif alpha_2 == 'WL':\n",
    "        return ROW_AMERICA\n",
    "    elif alpha_2 == 'WA':\n",
    "        return ROW_ASIA\n",
    "    elif alpha_2 == 'WE':\n",
    "        return ROW_EUROPE\n",
    "    elif alpha_2 == 'WM':\n",
    "        return ROW_ME\n",
    "    else: \n",
    "        country = pycountry.countries.get(alpha_2=alpha_2)\n",
    "        if country:\n",
    "            return pycountry.countries.get(alpha_2=alpha_2).name\n",
    "        else:\n",
    "            return 'No country'\n",
    "\n",
    "def scale_val(row, category_percentiles, global_upper_limit, global_lower_limit):\n",
    "    sector = row[EF_CAT_L2]\n",
    "    value = row[IPCC_AR5]\n",
    "    local_lower_limit, local_upper_limit = category_percentiles.loc[category_percentiles[EF_CAT_L2] == sector][IPCC_AR5].values\n",
    "\n",
    "    if value < 0 and value > -global_upper_limit:\n",
    "        return [-value, True]\n",
    "\n",
    "    if value < global_lower_limit and local_lower_limit < global_lower_limit:\n",
    "        return [global_lower_limit, True]\n",
    "    elif value > global_upper_limit and local_upper_limit > global_upper_limit:\n",
    "        return [global_upper_limit, True]\n",
    "\n",
    "    if value < local_lower_limit and not local_lower_limit == 0:\n",
    "        return [local_lower_limit, True]\n",
    "    elif value > local_upper_limit:\n",
    "        return [local_upper_limit, True]\n",
    "    elif value == 0 or value == np.nan:\n",
    "        return [global_lower_limit, True]\n",
    "\n",
    "    return [value, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load full raw database and create Ignite version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot insert EF Region, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m intermediate_df \u001b[39m=\u001b[39m m_t\u001b[39m.\u001b[39mmerge(right\u001b[39m=\u001b[39mx, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m, on\u001b[39m=\u001b[39m[EF_REGION, EF_CAT_L2])\n\u001b[1;32m     67\u001b[0m intermediate_df[number_columns] \u001b[39m=\u001b[39m intermediate_df[number_columns]\u001b[39m.\u001b[39mastype(\u001b[39mfloat\u001b[39m)\u001b[39m.\u001b[39mmultiply(intermediate_df[\u001b[39m'\u001b[39m\u001b[39msector_weights\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m, )\n\u001b[0;32m---> 69\u001b[0m region_averages \u001b[39m=\u001b[39m intermediate_df\u001b[39m.\u001b[39mgroupby([EF_REGION,EF_REGION, EF_CURR, EF_SOURCE, EF_YEAR])[number_columns]\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mreset_index()\n\u001b[1;32m     70\u001b[0m region_averages[[EF_CAT_L1, EF_CAT_L2]] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRegional average\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     71\u001b[0m region_averages[ADJUSTED] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/util/_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[1;32m    316\u001b[0m     )\n\u001b[0;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:6353\u001b[0m, in \u001b[0;36mDataFrame.reset_index\u001b[0;34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001b[0m\n\u001b[1;32m   6347\u001b[0m         \u001b[39mif\u001b[39;00m lab \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   6348\u001b[0m             \u001b[39m# if we have the codes, extract the values with a mask\u001b[39;00m\n\u001b[1;32m   6349\u001b[0m             level_values \u001b[39m=\u001b[39m algorithms\u001b[39m.\u001b[39mtake(\n\u001b[1;32m   6350\u001b[0m                 level_values, lab, allow_fill\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fill_value\u001b[39m=\u001b[39mlev\u001b[39m.\u001b[39m_na_value\n\u001b[1;32m   6351\u001b[0m             )\n\u001b[0;32m-> 6353\u001b[0m         new_obj\u001b[39m.\u001b[39;49minsert(\n\u001b[1;32m   6354\u001b[0m             \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   6355\u001b[0m             name,\n\u001b[1;32m   6356\u001b[0m             level_values,\n\u001b[1;32m   6357\u001b[0m             allow_duplicates\u001b[39m=\u001b[39;49mallow_duplicates,\n\u001b[1;32m   6358\u001b[0m         )\n\u001b[1;32m   6360\u001b[0m new_obj\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m new_index\n\u001b[1;32m   6361\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m inplace:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:4807\u001b[0m, in \u001b[0;36mDataFrame.insert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   4801\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   4802\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot specify \u001b[39m\u001b[39m'\u001b[39m\u001b[39mallow_duplicates=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4803\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mself.flags.allows_duplicate_labels\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is False.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4804\u001b[0m     )\n\u001b[1;32m   4805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_duplicates \u001b[39mand\u001b[39;00m column \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns:\n\u001b[1;32m   4806\u001b[0m     \u001b[39m# Should this be a different kind of error??\u001b[39;00m\n\u001b[0;32m-> 4807\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcannot insert \u001b[39m\u001b[39m{\u001b[39;00mcolumn\u001b[39m}\u001b[39;00m\u001b[39m, already exists\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   4808\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(loc, \u001b[39mint\u001b[39m):\n\u001b[1;32m   4809\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mloc must be int\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot insert EF Region, already exists"
     ]
    }
   ],
   "source": [
    "firstRun = True\n",
    "temp_m = False\n",
    "\n",
    "for year in range(2010, 2023):\n",
    "    for data_type in ['pxp']:\n",
    "        # not a very elegant solution, but it works\n",
    "        if firstRun:\n",
    "            firstRun = False\n",
    "        else:\n",
    "            temp_m = m_t\n",
    "\n",
    "        SOURCE_FOLDER = 'raw-data/' + '/IOT_' + str(year) + '_' + data_type\n",
    "\n",
    "        # load the Exiobase MRIO extension multipliers (M.txt) and gross/total output (x.txt) for the selected year and data_type\n",
    "        m = pd.read_csv(SOURCE_FOLDER + '/impacts/M.txt', sep='\\t', low_memory=False)\n",
    "        x = pd.read_csv(SOURCE_FOLDER + '/x.txt', sep='\\t')\n",
    "\n",
    "        # select the relevant rows and transpose matrix\n",
    "        m_t=m[m.region.isin(['sector', 'Water Consumption Blue - Total',\n",
    "                        'GHG emissions AR5 (GWP100) | GWP100 (IPCC, 2010)', \n",
    "                        'Land use Crop, Forest, Pasture'])].transpose().reset_index()\n",
    "\n",
    "        m_t.columns = ['region', 'sector','Water Consumption Blue - Total','GHG emissions AR5 (GWP100) | GWP100 (IPCC, 2010)', 'Land use Crop, Forest, Pasture']\n",
    "\n",
    "        # remove first row which doesn't have any valuable information after transposing\n",
    "        m_t = m_t.iloc[1:-1]\n",
    "\n",
    "        # change from M€ to €\n",
    "        m_t[IPCC_AR5]=(m_t['GHG emissions AR5 (GWP100) | GWP100 (IPCC, 2010)'].astype(float)/1000000).round(decimals = 8)\n",
    "\n",
    "        # add full region/ROW name based on alpha 2/region naming\n",
    "        m_t[EF_REGION] = m_t['region'].astype(str).apply(add_full_region_name)\n",
    "\n",
    "        # rename and add relevant columns\n",
    "        m_t.rename(columns={'Water Consumption Blue - Total': WATER, 'Land use Crop, Forest, Pasture': LAND}, inplace=True)\n",
    "        m_t[EF_YEAR] = str(year)\n",
    "        m_t[EF_CURR] = 'EUR'\n",
    "        m_t[EF_SOURCE] = 'Exiobase ' + EXIOBASE_VERSION + ' (Ignite update ' + IGNITE_VERSION + ')'\n",
    "        m_t[EF_CAT_L1] = 'Product'\n",
    "\n",
    "        # clean up the naming convention of Exiobase\n",
    "        m_t[EF_CAT_L2] = m_t['sector'].str.replace(r'\\([^)]\\d[^)]*\\)', '', regex=True).str.strip()\n",
    "        m_t[EF_CAT_L2] = m_t[EF_CAT_L2].str.replace('products of Vegetable oils and fats', 'Products of vegetable oils and fats')\n",
    "\n",
    "        # added to have a single text description of the unique combination of dimentions for the select emission factor\n",
    "        m_t[EF_UNIQUE] = m_t[EF_CAT_L1].astype(str) + '-' + m_t[EF_CAT_L2].astype(str) + '-' + m_t[EF_REGION].astype(str) + '-' + m_t[EF_YEAR].astype(str)\n",
    "        \n",
    "        # set to False for all emission factors initially, then set to True if updated at a later stage\n",
    "        m_t[ADJUSTED] = False\n",
    "\n",
    "        # remove columns not longer needed\n",
    "        m_t.drop(columns=['region', 'sector', 'GHG emissions AR5 (GWP100) | GWP100 (IPCC, 2010)'], inplace=True)\n",
    "\n",
    "        # limit outliers \n",
    "        global_lower_limit, global_upper_limit = m_t[IPCC_AR5].replace(0, np.nan).quantile([0.03,0.97]).values\n",
    "        category_percentiles = m_t.replace(0, np.nan).groupby(EF_CAT_L2)[IPCC_AR5].quantile([0.05, 0.95]).reset_index()\n",
    "        m_t[[EMISSION_FACTOR, ADJUSTED]] = m_t[[EF_CAT_L2, IPCC_AR5]].apply(lambda row: scale_val(row, category_percentiles, global_upper_limit, global_lower_limit), axis=1, result_type='expand').values\n",
    "\n",
    "        # add regional weighted averages\n",
    "        x.rename(columns={'region':EF_REGION, 'sector':EF_CAT_L2}, inplace=True)\n",
    "        x[EF_CAT_L2] = x[EF_CAT_L2].str.replace(r'\\([^)]\\d[^)]*\\)', '', regex=True).str.strip()\n",
    "        x[EF_CAT_L2] = x[EF_CAT_L2].str.replace('products of Vegetable oils and fats', 'Products of vegetable oils and fats')\n",
    "        x['sector_weights'] = x['indout'] / x.groupby(EF_REGION)['indout'].transform('sum')\n",
    "\n",
    "        number_columns = [WATER, LAND, IPCC_AR5, EMISSION_FACTOR]\n",
    "        intermediate_df = m_t.merge(right=x, how='left', on=[EF_REGION, EF_CAT_L2])\n",
    "        intermediate_df[number_columns] = intermediate_df[number_columns].astype(float).multiply(intermediate_df['sector_weights'], axis='index', )\n",
    "      \n",
    "        region_averages = intermediate_df.groupby([EF_REGION, EF_CURR, EF_SOURCE, EF_YEAR])[number_columns].sum().reset_index()\n",
    "        region_averages[[EF_CAT_L1, EF_CAT_L2]] = 'Regional average'\n",
    "        region_averages[ADJUSTED] = True\n",
    "        region_averages[EF_UNIQUE] = region_averages[EF_CAT_L1].astype(str) + '-' + region_averages[EF_REGION].astype(str) + '-' + region_averages[EF_YEAR].astype(str)\n",
    "\n",
    "        # set dtype of boolean column\n",
    "        m_t[ADJUSTED]=m_t[ADJUSTED].astype(bool)\n",
    "\n",
    "        # add regional averages for the selected year\n",
    "        m_t = pd.concat([m_t, region_averages], ignore_index=True)\n",
    "\n",
    "        # add data to the already run years if not first year\n",
    "        if type(temp_m) != bool:\n",
    "            m_t = pd.concat([temp_m, m_t], ignore_index=True)\n",
    "\n",
    "        print('Completed year:', year)\n",
    "\n",
    "# reorder columns\n",
    "m_t = m_t[[EF_CAT_L1, EF_CAT_L2, EF_REGION, EF_YEAR, EF_CURR, EF_SOURCE, EF_UNIQUE, ADJUSTED,  IPCC_AR5, WATER, LAND, EMISSION_FACTOR]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the completed file (EUR-version) in the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_t.to_excel('EUR_Exiobase3_8_2-Ignite1_1-Products_2010-2022.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
